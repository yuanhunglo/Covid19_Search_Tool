{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CORD19 - NLP Challenge",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuanhunglo/Covid19_Search_Tool/blob/master/CORD19_search_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cepc_sQETKhK",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqI2rescS2Ne",
        "colab_type": "code",
        "outputId": "1e23de6d-34b6-4957-a4ab-b80b524d6e37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pathlib import Path, PurePath\n",
        "import pandas as pd\n",
        "import requests\n",
        "from requests.exceptions import HTTPError, ConnectionError\n",
        "from ipywidgets import interact\n",
        "import ipywidgets as widgets\n",
        "import re\n",
        "from ipywidgets import interact\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPoA0gobj4ck",
        "colab_type": "text"
      },
      "source": [
        "### Connect to personal google drive google drive to enable data download\n",
        "Requires you to have this data available on your personal drive: https://drive.google.com/open?id=1ZVxvPnrnA8ffGoFsVxJs75QL9li6AfG7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skzxBe_NjzII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive # for connecting to dataset on personal google drive\n",
        "# mount personal google drive that has data uploaded (Requires verification)\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C076btW_cSYW",
        "colab_type": "text"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mMO08ivinJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# upload data and list contents\n",
        "input_dir = PurePath('/content/drive/My Drive/CORD-19-research-challenge')\n",
        "list(Path(input_dir).glob('*'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhqGnR0GjsxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metadata_path = input_dir / 'metadata.csv'\n",
        "metadata = pd.read_csv(metadata_path,\n",
        "                               dtype={'Microsoft Academic Paper ID': str,\n",
        "                                      'pubmed_id': str})\n",
        "\n",
        "# Set the abstract to the paper title if it is null\n",
        "metadata.abstract = metadata.abstract.fillna(metadata.title)\n",
        "print(\"Number of articles before removing duplicates: %s \" % len(metadata))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hJqOLjmTUaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some papers are duplicated since they were collected from separate sources. Thanks Joerg Rings\n",
        "duplicate_paper = ~(metadata.title.isnull() | metadata.abstract.isnull() | metadata.publish_time.isnull()) & (metadata.duplicated(subset=['title', 'abstract']))\n",
        "metadata.dropna(subset=['publish_time', 'journal'])\n",
        "metadata = metadata[~duplicate_paper].reset_index(drop=True)\n",
        "print(\"Number of articles AFTER removing duplicates: %s \" % len(metadata))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr-64L0GEB2l",
        "colab_type": "text"
      },
      "source": [
        "### **TODO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfGQHLQOBwCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# REMOVE articles missing publish_date or journal name\n",
        "print(\"Number of articles AFTER removing missing date and journal: %s \" % len(metadata))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3SQuCAvkjsq",
        "colab_type": "text"
      },
      "source": [
        "### Create Data Classes for the Research Dataset and Papers\n",
        "These classes make it easier to navigate through the datasources. There is a class called ResearchPapers that wraps the entire dataset an provide useful functions to navigate through it, and Paper, that make it easier to view each paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02vqttFfcYcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get(url, timeout=6):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=timeout)\n",
        "        return r.text\n",
        "    except ConnectionError:\n",
        "        print(f'Cannot connect to {url}')\n",
        "        print(f'Remember to turn Internet ON in the Kaggle notebook settings')\n",
        "    except HTTPError:\n",
        "        print('Got http error', r.status, r.text)\n",
        "\n",
        "# Convert the doi to a url\n",
        "def doi_url(d): \n",
        "    return f'http://{d}' if d.startswith('doi.org') else f'http://doi.org/{d}'\n",
        "\n",
        "\n",
        "class ResearchPapers:\n",
        "    \n",
        "    def __init__(self, metadata: pd.DataFrame):\n",
        "        self.metadata = metadata\n",
        "        \n",
        "    def __getitem__(self, item):\n",
        "        return Paper(self.metadata.iloc[item])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "    \n",
        "    def head(self, n):\n",
        "        return ResearchPapers(self.metadata.head(n).copy().reset_index(drop=True))\n",
        "    \n",
        "    def tail(self, n):\n",
        "        return ResearchPapers(self.metadata.tail(n).copy().reset_index(drop=True))\n",
        "    \n",
        "    def abstracts(self):\n",
        "        return self.metadata.abstract.dropna()\n",
        "    \n",
        "    def titles(self):\n",
        "        return self.metadata.title.dropna()\n",
        "        \n",
        "    def _repr_html_(self):\n",
        "        return self.metadata._repr_html_()\n",
        "    \n",
        "class Paper:\n",
        "    \n",
        "    '''\n",
        "    A single research paper\n",
        "    '''\n",
        "    def __init__(self, item):\n",
        "        self.paper = item.to_frame().fillna('')\n",
        "        self.paper.columns = ['Value']\n",
        "    \n",
        "    def doi(self):\n",
        "        return self.paper.loc['doi'].values[0]\n",
        "    \n",
        "    def html(self):\n",
        "        '''\n",
        "        Load the paper from doi.org and display as HTML. Requires internet to be ON\n",
        "        '''\n",
        "        if self.doi():\n",
        "            url = doi_url(self.doi()) \n",
        "            text = get(url)\n",
        "            return widgets.HTML(text)\n",
        "    \n",
        "    def text(self):\n",
        "        '''\n",
        "        Load the paper from doi.org and display as text. Requires Internet to be ON\n",
        "        '''\n",
        "        text = get(self.doi())\n",
        "        return text\n",
        "    \n",
        "    def abstract(self):\n",
        "        return self.paper.loc['abstract'].values[0]\n",
        "    \n",
        "    def title(self):\n",
        "        return self.paper.loc['title'].values[0]\n",
        "    \n",
        "    def authors(self, split=False):\n",
        "        '''\n",
        "        Get a list of authors\n",
        "        '''\n",
        "        authors = self.paper.loc['authors'].values[0]\n",
        "        if not authors:\n",
        "            return []\n",
        "        if not split:\n",
        "            return authors\n",
        "        if authors.startswith('['):\n",
        "            authors = authors.lstrip('[').rstrip(']')\n",
        "            return [a.strip().replace(\"\\'\", \"\") for a in authors.split(\"\\',\")]\n",
        "        \n",
        "        # Todo: Handle cases where author names are separated by \",\"\n",
        "        return [a.strip() for a in authors.split(';')]\n",
        "        \n",
        "    def _repr_html_(self):\n",
        "        return self.paper._repr_html_()\n",
        "    \n",
        "\n",
        "papers = ResearchPapers(metadata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuVbNpd5nPxv",
        "colab_type": "text"
      },
      "source": [
        "#### Show a Paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAQ6vb9ynO3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "papers[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgoC-KGBnJwZ",
        "colab_type": "text"
      },
      "source": [
        "#### Pull info from a paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snv1YrO4cY_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index=1\n",
        "paper=papers[index]\n",
        "print(\"Example paper #%s\\nTitle: %s\\nAuthors: %s \" % (index, paper.title(), paper.authors(split=True)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLdUa4VRn_a_",
        "colab_type": "text"
      },
      "source": [
        "### Text Preprocessing\n",
        "To prepare the text for the search index we perform the following steps\n",
        "1.   Remove punctuations and special characters\n",
        "2.   Convert to lowercase\n",
        "3.   Tokenize into individual tokens (words mostly)\n",
        "4.   Remove stopwords like (and, to))\n",
        "5.   Lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcGK5UOApijH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the stop words we plan on using\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxTjWknWprdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hardcode the data we want to use in search\n",
        "SEARCH_DISPLAY_COLUMNS = ['title', 'abstract', 'doi', 'authors', 'journal', 'publish_time']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxPK2uLelsZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_stopwords = list(set(stopwords.words('english')))\n",
        "\n",
        "def strip_characters(text):\n",
        "    t = re.sub('\\(|\\)|:|,|;|\\.|’|”|“|\\?|%|>|<', '', text)\n",
        "    t = re.sub('/', ' ', t)\n",
        "    t = t.replace(\"'\",'')\n",
        "    return t\n",
        "\n",
        "def clean(text):\n",
        "    t = text.lower()\n",
        "    t = strip_characters(t)\n",
        "    return t\n",
        "\n",
        "def tokenize(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return list(set([word for word in words \n",
        "                     if len(word) > 1\n",
        "                     and not word in english_stopwords\n",
        "                     and not (word.isnumeric() and len(word) is not 4)\n",
        "                     and (not word.isnumeric() or word.isalpha())] )\n",
        "               )\n",
        "    \n",
        "def lemmatize(word_list,lemmatizer):\n",
        "    # Init the Wordnet Lemmatizer\n",
        "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "    return lemmatized_output\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    t = clean(text)\n",
        "    tokens = tokenize(t)\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    tokens = lemmatize(tokens,lemmatizer)\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQlY0SyEoUIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SearchResults:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 data: pd.DataFrame,\n",
        "                 columns = None):\n",
        "        self.results = data\n",
        "        if columns:\n",
        "            self.results = self.results[columns]\n",
        "            \n",
        "    def __getitem__(self, item):\n",
        "        return Paper(self.results.loc[item])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.results)\n",
        "        \n",
        "    def _repr_html_(self):\n",
        "        return self.results._repr_html_()\n",
        "\n",
        "class WordTokenIndex:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 corpus: pd.DataFrame, \n",
        "                 columns=SEARCH_DISPLAY_COLUMNS):\n",
        "        self.corpus = corpus\n",
        "        raw_search_str = self.corpus.abstract.fillna('') + ' ' + self.corpus.title.fillna('')\n",
        "        self.index = raw_search_str.apply(preprocess).to_frame()\n",
        "        self.index.columns = ['terms']\n",
        "        self.index.index = self.corpus.index\n",
        "        self.columns = columns\n",
        "\n",
        "    \n",
        "    def search(self, search_string):\n",
        "        search_terms = preprocess(search_string)\n",
        "        result_index = self.index.terms.apply(lambda terms: any(i in terms for i in search_terms))\n",
        "        results = self.corpus[result_index].copy().reset_index().rename(columns={'index':'paper'})\n",
        "        return SearchResults(results, self.columns + ['paper'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDnsymvcnak9",
        "colab_type": "text"
      },
      "source": [
        "### Creating a search index¶ - Using a RankBM25 Search Index\n",
        "We will create a simple search index that will just match search tokens in a document. First we tokenize the abstract and store it in a dataframe. Then we just match search terms against it.\n",
        "\n",
        "RankBM25 is a python library that implements algorithms for a simple search index. https://pypi.org/project/rank-bm25/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm9PF76UlsN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install rank_bm25\n",
        "# Create a prebaked search engine with existing package: https://pypi.org/project/rank-bm25/\n",
        "from rank_bm25 import BM25Okapi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa0BSZXFo9OB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RankBM25Index(WordTokenIndex):\n",
        "    \n",
        "    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n",
        "        super().__init__(corpus, columns)\n",
        "        self.bm25 = BM25Okapi(self.index.terms.tolist())\n",
        "        \n",
        "    def search(self, search_string, n=4):\n",
        "        search_terms = preprocess(search_string)\n",
        "        doc_scores = self.bm25.get_scores(search_terms)\n",
        "        ind = np.argsort(doc_scores)[::-1][:n]\n",
        "        results = self.corpus.iloc[ind][self.columns]\n",
        "        results['Score'] = doc_scores[ind]\n",
        "        results = results[results.Score > 0]\n",
        "        return SearchResults(results.reset_index(), self.columns + ['Score'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQqNtiMb-3rc",
        "colab_type": "text"
      },
      "source": [
        "### Create the index (This takes several minutes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUsxYcrUobbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bm25_index = RankBM25Index(metadata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOwEu4pgGb7n",
        "colab_type": "text"
      },
      "source": [
        "### Search by date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4RubsMNpGK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example output\n",
        "query='curise ship'\n",
        "n=50\n",
        "results = bm25_index.search(query,n)\n",
        "results.results.sort_values(by=['publish_time'], ascending=False).head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulCpY3ooEr4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example output\n",
        "query='ACE spike'\n",
        "n=50\n",
        "results = bm25_index.search(query,n)\n",
        "results.results.sort_values(by=['publish_time'], ascending=False).head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQLY3TWUAnG3",
        "colab_type": "text"
      },
      "source": [
        "### Creating an Autocomplete Search bar with ranking by score\n",
        "Here we provide a search bar with autocomplete. This uses IPywidgets interactive rendering of a TextBox."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beMz_ebQAr74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_papers(SearchTerms: str):\n",
        "    results_to_consider=200\n",
        "    results_to_display=10\n",
        "    # gather search results by score\n",
        "    output = bm25_index.search(SearchTerms, n=results_to_consider)\n",
        "    # sort results by recency\n",
        "    # output=search_results.results.sort_values(by=['publish_time'], ascending=False).head(results_to_display)\n",
        "    if len(output) > 0:\n",
        "        display(output) \n",
        "    return output\n",
        "\n",
        "searchbar = widgets.interactive(search_papers, SearchTerms='ACE spike')\n",
        "searchbar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFTCTL0UGLlM",
        "colab_type": "text"
      },
      "source": [
        "### TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77C-cve8GMmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do search with option to restrict years available"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvkuSN65_spU",
        "colab_type": "text"
      },
      "source": [
        "### Looking at the Covid Research Tasks\n",
        "This dataset has a number of tasks. We will try to organize the papers according to the tasks\n",
        "\n",
        "What is known about transmission, incubation, and environmental stability?\n",
        "What do we know about COVID-19 risk factors?\n",
        "What do we know about virus genetics, origin, and evolution?\n",
        "What has been published about ethical and social science considerations?\n",
        "What do we know about diagnostics and surveillance?\n",
        "What has been published about medical care?\n",
        "What do we know about non-pharmaceutical interventions?\n",
        "What has been published about information sharing and inter-sectoral collaboration?\n",
        "What do we know about vaccines and therapeutics?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCAtXGvh_XAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tasks = [('What is known about transmission, incubation, and environmental stability?', \n",
        "        'transmission incubation environment coronavirus'),\n",
        "        ('What do we know about COVID-19 risk factors?', 'risk factors'),\n",
        "        ('What do we know about virus genetics, origin, and evolution?', 'genetics origin evolution'),\n",
        "        ('What has been published about ethical and social science considerations','ethics ethical social'),\n",
        "        ('What do we know about diagnostics and surveillance?','diagnose diagnostic surveillance'),\n",
        "        ('What has been published about medical care?', 'medical care'),\n",
        "        ('What do we know about vaccines and therapeutics?', 'vaccines vaccine vaccinate therapeutic therapeutics')] \n",
        "tasks = pd.DataFrame(tasks, columns=['Task', 'Keywords'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_2xnawh_z98",
        "colab_type": "text"
      },
      "source": [
        "#### Research papers for each task\n",
        "Here we add a dropdown that allows for selection of tasks and show the search results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cDtB9ggPFcMq",
        "colab": {}
      },
      "source": [
        "def show_task(Task):\n",
        "    print(Task)\n",
        "    keywords = tasks[tasks.Task == Task].Keywords.values[0]\n",
        "    search_results = bm25_index.search(keywords, n=200)\n",
        "    return search_results\n",
        "    \n",
        "results = interact(show_task, Task = tasks.Task.tolist());"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67PH21y__XYn",
        "colab_type": "text"
      },
      "source": [
        "# Create a BERT sentance encoding search engine \n",
        "From: https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a\n",
        "By: Denis Antyukhov\n",
        "In this experiment, we will use a pre-trained BERT model checkpoint to build a general-purpose text feature extractor.\n",
        "\n",
        "These things are sometimes referred to as Natural Language Understanding (NLU) modules, because the features they extract are relevant for a wide array of downstream NLP tasks.\n",
        "\n",
        "One use for these features is in instance-based learning, which relies on computing the similarity of the query to the training samples.\n",
        "\n",
        "We will illustrate this by building a simple Information Retrieval system using the BERT NLU module for feature extraction.\n",
        "\n",
        "**The plan for this experiment is:**\n",
        "1. getting the pre-trained BERT model checkpoint\n",
        "2. extracting a sub-graph optimized for inference\n",
        "3. creating a feature extractor with tf.Estimator\n",
        "4. exploring vector space with T-SNE and Embedding Projector\n",
        "5. implementing an Information Retrieval engine\n",
        "6. accelerating search queries with math\n",
        "7. building a covid research article recommendation system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbCgocXVJ4bY",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: getting the pre-trained model\n",
        "We start with a pre-trained english BERT-base model checkpoint.\n",
        "\n",
        "For configuring and optimizing the graph for inference we will use bert-as-a-service repository, which allows for serving BERT models for remote clients over TCP.\n",
        "\n",
        "Having a remote BERT-server is beneficial in multi-host environments. However, in this part of the experiment we will focus on creating a local (in-process) feature extractor. This is useful if one wishes to avoid additional latency and potential failure modes introduced by a client-server architecture. Now, let us download the model and install the package.\n",
        "\n",
        "Now, let us download the model and install the package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obm1_Pti_gqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip\n",
        "!pip install bert-serving-server --no-deps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGlziSVYK0gy",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: optimizing the inference graph\n",
        "Normally, to modify the model graph we would have to do some low-level TensorFlow programming. \n",
        "\n",
        "However, thanks to bert-as-a-service, we can configure the inference graph using a simple CLI interface.\n",
        "\n",
        "There are a couple of parameters in the below snippet too look out for.\n",
        "\n",
        "For each text sample, BERT-base model encoding layers output a tensor of shape **[sequence_len, encoder_dim],** with one vector per input token. To obtain a fixed representation, we need to apply some sort of pooling.\n",
        "\n",
        "**POOL_STRAT** parameter defines the pooling strategy applied to the  **POOL_LAYER** encoding layer. The default value **REDUCE_MEAN** averages the vectors for all tokens in a sequence. This strategy works best for most sentence-level tasks, when the model is not fine-tuned. Another option is NONE, in which case no pooling is applied at all. This is useful for word-level tasks such as Named Entity Recognition or POS tagging. For a detailed discussion of other options check out the Han Xiao's [blog post.](https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/)\n",
        "\n",
        "**SEQ_LEN** affects the maximum length of sequences processed by the model. Smaller values increase the model inference speed almost linearly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkYOjgI1_ep3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "sesh = tf.InteractiveSession()\n",
        "\n",
        "from bert_serving.server.graph import optimize_graph\n",
        "from bert_serving.server.helper import get_args_parser\n",
        "\n",
        "# input dir\n",
        "MODEL_DIR = '/content/uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "# output dir\n",
        "GRAPH_DIR = '/content/graph/' #@param {type:\"string\"}\n",
        "# output filename\n",
        "GRAPH_OUT = 'extractor.pbtxt' #@param {type:\"string\"}\n",
        "\n",
        "POOL_STRAT = 'REDUCE_MEAN' #@param ['REDUCE_MEAN', 'REDUCE_MAX', \"NONE\"]\n",
        "POOL_LAYER = '-2' #@param {type:\"string\"}\n",
        "SEQ_LEN = '256' #@param {type:\"string\"}\n",
        "\n",
        "tf.gfile.MkDir(GRAPH_DIR)\n",
        "\n",
        "parser = get_args_parser()\n",
        "carg = parser.parse_args(args=['-model_dir', MODEL_DIR,\n",
        "                               '-graph_tmp_dir', GRAPH_DIR,\n",
        "                               '-max_seq_len', str(SEQ_LEN),\n",
        "                               '-pooling_layer', str(POOL_LAYER),\n",
        "                               '-pooling_strategy', POOL_STRAT])\n",
        "\n",
        "tmp_name, config = optimize_graph(carg)\n",
        "graph_fout = os.path.join(GRAPH_DIR, GRAPH_OUT)\n",
        "\n",
        "tf.gfile.Rename(\n",
        "    tmp_name,\n",
        "    graph_fout,\n",
        "    overwrite=True\n",
        ")\n",
        "print(\"\\nSerialized graph to {}\".format(graph_fout))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q2yDu4wLAkN",
        "colab_type": "text"
      },
      "source": [
        "Running the above snippet will put the BERT model graph and weights from  **MODEL_DIR** into a GraphDef object which will be serialized to a pbtxt file at **GRAPH_OUT**. The file will be smaller than the original model because the nodes and variables required for training will be removed. This results in a quite portable solution: for example the english base model only takes 389 MB after exporting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eARzl4sCLI7d",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: creating a feature extractor\n",
        "Now, we will use the serialized graph to build a feature extractor using the tf.Estimator API. We will need to define two things: **input_fn** and **model_fn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFrwq2AZLCkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.python.estimator.estimator import Estimator\n",
        "from tensorflow.python.estimator.run_config import RunConfig\n",
        "from tensorflow.python.estimator.model_fn import EstimatorSpec\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "from bert_serving.server.bert.tokenization import FullTokenizer\n",
        "from bert_serving.server.bert.extract_features import convert_lst_to_features\n",
        "\n",
        "\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "log.handlers = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zM-IaRxLNmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GRAPH_PATH = \"/content/graph/extractor.pbtxt\" #@param {type:\"string\"}\n",
        "VOCAB_PATH = \"/content/uncased_L-12_H-768_A-12/vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "SEQ_LEN = 256 #@param {type:\"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z885uqROLSN6",
        "colab_type": "text"
      },
      "source": [
        "**input_fn** manages getting the data into the model. That includes executing the whole text preprocessing pipeline and preparing a feed_dict for BERT. \n",
        "\n",
        "First, each text sample is converted into a tf.Example instance containing the necessary features listed in **INPUT_NAMES**. The bert_tokenizer object contains  the WordPiece vocabulary and performs the text preprocessing. After that the examples are re-grouped by feature name in a **feed_dict**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-SMkDVYLP1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_NAMES = ['input_ids', 'input_mask', 'input_type_ids']\n",
        "bert_tokenizer = FullTokenizer(VOCAB_PATH)\n",
        "\n",
        "def build_feed_dict(texts):\n",
        "    \n",
        "    text_features = list(convert_lst_to_features(\n",
        "        texts, SEQ_LEN, SEQ_LEN, \n",
        "        bert_tokenizer, log, False, False))\n",
        "\n",
        "    target_shape = (len(texts), -1)\n",
        "\n",
        "    feed_dict = {}\n",
        "    for iname in INPUT_NAMES:\n",
        "        features_i = np.array([getattr(f, iname) for f in text_features])\n",
        "        features_i = features_i.reshape(target_shape).astype(\"int32\")\n",
        "        feed_dict[iname] = features_i\n",
        "\n",
        "    return feed_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpSMbxKKLYVe",
        "colab_type": "text"
      },
      "source": [
        "tf.Estimators have a fun feature which makes them re-build and re-initialize the whole computational graph at each call to the predict function. \n",
        "\n",
        "So, in order to avoid the overhead, to the predict function we will pass a generator, which will yield the features to the model in a never-ending loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gne1y7etLaEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_input_fn(container):\n",
        "    \n",
        "    def gen():\n",
        "        while True:\n",
        "          try:\n",
        "            yield build_feed_dict(container.get())\n",
        "          except:\n",
        "            yield build_feed_dict(container.get())\n",
        "\n",
        "    def input_fn():\n",
        "        return tf.data.Dataset.from_generator(\n",
        "            gen,\n",
        "            output_types={iname: tf.int32 for iname in INPUT_NAMES},\n",
        "            output_shapes={iname: (None, None) for iname in INPUT_NAMES})\n",
        "    return input_fn\n",
        "\n",
        "class DataContainer:\n",
        "  def __init__(self):\n",
        "    self._texts = None\n",
        "  \n",
        "  def set(self, texts):\n",
        "    if type(texts) is str:\n",
        "      texts = [texts]\n",
        "    self._texts = texts\n",
        "    \n",
        "  def get(self):\n",
        "    return self._texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG8MrQ4ILc-l",
        "colab_type": "text"
      },
      "source": [
        "**model_fn** contains the specification of the model. In our case, it is loaded from the pbtxt file we saved in the previous step. \n",
        "\n",
        "The features are mapped explicitly to the corresponding input nodes with input_map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8m5Uch7Lf9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, mode):\n",
        "    with tf.gfile.GFile(GRAPH_PATH, 'rb') as f:\n",
        "        graph_def = tf.GraphDef()\n",
        "        graph_def.ParseFromString(f.read())\n",
        "        \n",
        "    output = tf.import_graph_def(graph_def,\n",
        "                                 input_map={k + ':0': features[k] for k in INPUT_NAMES},\n",
        "                                 return_elements=['final_encodes:0'])\n",
        "\n",
        "    return EstimatorSpec(mode=mode, predictions={'output': output[0]})\n",
        "  \n",
        "estimator = Estimator(model_fn=model_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTEqJGX5LpXH",
        "colab_type": "text"
      },
      "source": [
        "Now we have everything we need to perform inference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYv18IqcLnQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch(iterable, n=1):\n",
        "    l = len(iterable)\n",
        "    for ndx in range(0, l, n):\n",
        "        yield iterable[ndx:min(ndx + n, l)]\n",
        "\n",
        "def build_vectorizer(_estimator, _input_fn_builder, batch_size=128):\n",
        "  container = DataContainer()\n",
        "  predict_fn = _estimator.predict(_input_fn_builder(container), yield_single_examples=False)\n",
        "  \n",
        "  def vectorize(text, verbose=False):\n",
        "    x = []\n",
        "    bar = Progbar(len(text))\n",
        "    for text_batch in batch(text, batch_size):\n",
        "      container.set(text_batch)\n",
        "      x.append(next(predict_fn)['output'])\n",
        "      if verbose:\n",
        "        bar.add(len(text_batch))\n",
        "      \n",
        "    r = np.vstack(x)\n",
        "    return r\n",
        "  \n",
        "  return vectorize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw33e3SRLnKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_vectorizer = build_vectorizer(estimator, build_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAAXDUrsLnGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_vectorizer(64*['sample text']).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_YuBehNMKvT",
        "colab_type": "text"
      },
      "source": [
        "### Step 4: exploring vector space with Projector\n",
        "\n",
        "*A* standalone version of BERT feature extractor is available in the [repository](https://github.com/gaphex/bert_experimental).\n",
        "\n",
        "Using the vectorizer we will generate embeddings for articles from the CORD-19 benchmark (in this tutorial, the Reuters-21578 benchmark corpus was used previously)\n",
        "\n",
        "To visualise and explore the embedding vector space in 3D we will use a dimensionality reduction technique called [T-SNE](https://distill.pub/2016/misread-tsne/).\n",
        "\n",
        "Lets get the article embeddings first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrWW7qxLLm9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "nltk.download(\"reuters\")\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jibjLu9Tx6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(reuters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pSFM8TOM1wU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# REUTERS EXAMPLE\n",
        "max_samples = 256\n",
        "categories = ['wheat', 'tea', 'strategic-metal', \n",
        "              'housing', 'money-supply', 'fuel']\n",
        "\n",
        "S, X, Y = [], [], []\n",
        "\n",
        "for category in categories:\n",
        "  print(category)\n",
        "  \n",
        "  sents = reuters.sents(categories=category)\n",
        "  sents = [' '.join(sent) for sent in sents][:max_samples]\n",
        "  X.append(bert_vectorizer(sents, verbose=True))\n",
        "  Y += [category] * len(sents)\n",
        "  S += sents\n",
        "  \n",
        "X = np.vstack(X) \n",
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSAPjrWkNPZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"embeddings.tsv\", \"w\") as fo:\n",
        "  for x in X.astype('float16'):\n",
        "    line = \"\\t\".join([str(v) for v in x])\n",
        "    fo.write(line + \"\\n\")\n",
        "\n",
        "with open(\"metadata.tsv\", \"w\") as fo:\n",
        "  fo.write(\"Label\\tSentence\\n\")\n",
        "  for y, s in zip(Y, S):\n",
        "    fo.write(\"{}\\t{}\\n\".format(y, s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqVYk769NiiB",
        "colab_type": "text"
      },
      "source": [
        "The interactive visualization of generated embeddings is available on the [Embedding Projector](https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/gaphex/7262af1e151957b1e7c638f4922dfe57/raw/3b946229fc58cbefbca2a642502cf51d4f8e81c5/reuters_proj_config.json). **<--CLICK THAT TO GENERATE**\n",
        "\n",
        "From the link you can run T-SNE yourself, or load a checkpoint using the bookmark in lower-right corner (loading works only on Chrome).\n",
        "\n",
        "To reproduce the input files used for this visualization, run the code below. Then, download the files to your machine and upload to Projector\n",
        "\n",
        "(you can dowload files from the menu opened by the \">\" button in the upper-left)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvqeITuSNjPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"900\" height=\"632\" controls>\n",
        "  <source src=\"https://storage.googleapis.com/bert_resourses/reuters_tsne_hd.mp4\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSYC8R9zUH3E",
        "colab_type": "text"
      },
      "source": [
        "### Create embeddings for CORD19 Articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNknqrSIUL5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert pandas dataframe to nltk.corpus.reader.plaintext.CategorizedPlaintextCorpusReader\n",
        "# From: https://stackoverflow.com/questions/49088978/how-to-create-corpus-from-pandas-data-frame-to-operate-with-nltk/49104725\n",
        "def CreateCorpusFromDataFrame(corpusfolder,df):\n",
        "    for index, r in df.iterrows():\n",
        "        id=index\n",
        "        title=r['title']\n",
        "        body=r['title']\n",
        "        # handler text for not properly munged data\n",
        "        try: \n",
        "          category=re.sub('/', '', r['journal']) # remove odd characters as writing to file\n",
        "        except TypeError:\n",
        "          continue\n",
        "        fname=str(category)+'_'+str(id)+'.txt'\n",
        "        corpusfile=open(corpusfolder+'/'+fname,'a+')\n",
        "        corpusfile.write(str(body) +\" \" +str(title))\n",
        "        corpusfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmAcjhVqV2kK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create folder to hold CORD19 nltk\n",
        "dirName = 'CORD19_nltk_title_only'\n",
        "try:\n",
        "    # Create target Directory\n",
        "    os.mkdir(dirName)\n",
        "except FileExistsError:\n",
        "    pass\n",
        "\n",
        "# create corpus\n",
        "CreateCorpusFromDataFrame(dirName,metadata)\n",
        "print(\"Corpus created in folder: %s\" % dirName)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yds9N-ggY4M2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the corpus reader\n",
        "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
        "\n",
        "# Create NLTK data structure (with pattern matching to create the article names again)\n",
        "CORD_corpus=CategorizedPlaintextCorpusReader(dirName,r'.*', cat_pattern=r'(.*)_.*.txt$') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kGn261zN8fZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# total journals\n",
        "print(\"Total number journals: %s\" % (len(metadata.journal.unique())))\n",
        "\n",
        "# select a subset of journals, where the journal will be the tag\n",
        "num_journals=8\n",
        "categories=metadata['journal'].value_counts()[:num_journals].index.tolist()\n",
        "print (\"\\nPicking most common journals:\")\n",
        "categories\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0jyk0_9LmmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CORD19 Examples\n",
        "max_samples = 5000\n",
        "\n",
        "S, X, Y = [], [], []\n",
        "\n",
        "for category in categories:\n",
        "  print(category)\n",
        "  \n",
        "  sents = CORD_corpus.sents(categories=category)\n",
        "  sents = [' '.join(sent) for sent in sents][:max_samples]\n",
        "  X.append(bert_vectorizer(sents, verbose=True))\n",
        "  Y += [category] * len(sents)\n",
        "  S += sents\n",
        "  \n",
        "X = np.vstack(X) \n",
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0M5K6rEnt-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make folder in google drive to download files\n",
        "location = '/content/drive/My Drive/'\n",
        "\n",
        "with open(location + \"embeddings_large.tsv\", \"w\") as fo:\n",
        "  for x in X.astype('float16'):\n",
        "    line = \"\\t\".join([str(v) for v in x])\n",
        "    fo.write(line + \"\\n\")\n",
        "\n",
        "with open(location + \"metadata_large.tsv\", \"w\") as fo:\n",
        "  fo.write(\"Label\\tSentence\\n\")\n",
        "  for y, s in zip(Y, S):\n",
        "    fo.write(\"{}\\t{}\\n\".format(y, s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3JbwudUnzrU",
        "colab_type": "text"
      },
      "source": [
        "The interactive visualization of generated embeddings is available on the [Embedding Projector](https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/gaphex/7262af1e151957b1e7c638f4922dfe57/raw/3b946229fc58cbefbca2a642502cf51d4f8e81c5/reuters_proj_config.json). **<--CLICK THAT TO GENERATE**\n",
        "\n",
        "Then go to bottom right and load in those files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jLduttHn2P1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}